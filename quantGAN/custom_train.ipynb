{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from vq_model import VQ_models\n",
    "from vq_loss import VQLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys (weights not loaded): ['encoder.conv_blocks.0.res.0.lora_conv1.lora_A.weight', 'encoder.conv_blocks.0.res.0.lora_conv1.lora_B.weight', 'encoder.conv_blocks.0.res.0.lora_conv2.lora_A.weight', 'encoder.conv_blocks.0.res.0.lora_conv2.lora_B.weight', 'encoder.conv_blocks.0.res.1.lora_conv1.lora_A.weight', 'encoder.conv_blocks.0.res.1.lora_conv1.lora_B.weight', 'encoder.conv_blocks.0.res.1.lora_conv2.lora_A.weight', 'encoder.conv_blocks.0.res.1.lora_conv2.lora_B.weight', 'encoder.conv_blocks.1.res.0.lora_conv1.lora_A.weight', 'encoder.conv_blocks.1.res.0.lora_conv1.lora_B.weight', 'encoder.conv_blocks.1.res.0.lora_conv2.lora_A.weight', 'encoder.conv_blocks.1.res.0.lora_conv2.lora_B.weight', 'encoder.conv_blocks.1.res.0.lora_nin_shortcut.lora_A.weight', 'encoder.conv_blocks.1.res.0.lora_nin_shortcut.lora_B.weight', 'encoder.conv_blocks.1.res.1.lora_conv1.lora_A.weight', 'encoder.conv_blocks.1.res.1.lora_conv1.lora_B.weight', 'encoder.conv_blocks.1.res.1.lora_conv2.lora_A.weight', 'encoder.conv_blocks.1.res.1.lora_conv2.lora_B.weight', 'encoder.conv_blocks.2.res.0.lora_conv1.lora_A.weight', 'encoder.conv_blocks.2.res.0.lora_conv1.lora_B.weight', 'encoder.conv_blocks.2.res.0.lora_conv2.lora_A.weight', 'encoder.conv_blocks.2.res.0.lora_conv2.lora_B.weight', 'encoder.conv_blocks.2.res.1.lora_conv1.lora_A.weight', 'encoder.conv_blocks.2.res.1.lora_conv1.lora_B.weight', 'encoder.conv_blocks.2.res.1.lora_conv2.lora_A.weight', 'encoder.conv_blocks.2.res.1.lora_conv2.lora_B.weight', 'encoder.conv_blocks.3.res.0.lora_conv1.lora_A.weight', 'encoder.conv_blocks.3.res.0.lora_conv1.lora_B.weight', 'encoder.conv_blocks.3.res.0.lora_conv2.lora_A.weight', 'encoder.conv_blocks.3.res.0.lora_conv2.lora_B.weight', 'encoder.conv_blocks.3.res.0.lora_nin_shortcut.lora_A.weight', 'encoder.conv_blocks.3.res.0.lora_nin_shortcut.lora_B.weight', 'encoder.conv_blocks.3.res.1.lora_conv1.lora_A.weight', 'encoder.conv_blocks.3.res.1.lora_conv1.lora_B.weight', 'encoder.conv_blocks.3.res.1.lora_conv2.lora_A.weight', 'encoder.conv_blocks.3.res.1.lora_conv2.lora_B.weight', 'encoder.conv_blocks.3.attn.0.lora_q.lora_A.weight', 'encoder.conv_blocks.3.attn.0.lora_q.lora_B.weight', 'encoder.conv_blocks.3.attn.0.lora_k.lora_A.weight', 'encoder.conv_blocks.3.attn.0.lora_k.lora_B.weight', 'encoder.conv_blocks.3.attn.0.lora_v.lora_A.weight', 'encoder.conv_blocks.3.attn.0.lora_v.lora_B.weight', 'encoder.conv_blocks.3.attn.1.lora_q.lora_A.weight', 'encoder.conv_blocks.3.attn.1.lora_q.lora_B.weight', 'encoder.conv_blocks.3.attn.1.lora_k.lora_A.weight', 'encoder.conv_blocks.3.attn.1.lora_k.lora_B.weight', 'encoder.conv_blocks.3.attn.1.lora_v.lora_A.weight', 'encoder.conv_blocks.3.attn.1.lora_v.lora_B.weight', 'encoder.mid.0.lora_conv1.lora_A.weight', 'encoder.mid.0.lora_conv1.lora_B.weight', 'encoder.mid.0.lora_conv2.lora_A.weight', 'encoder.mid.0.lora_conv2.lora_B.weight', 'encoder.mid.1.lora_q.lora_A.weight', 'encoder.mid.1.lora_q.lora_B.weight', 'encoder.mid.1.lora_k.lora_A.weight', 'encoder.mid.1.lora_k.lora_B.weight', 'encoder.mid.1.lora_v.lora_A.weight', 'encoder.mid.1.lora_v.lora_B.weight', 'encoder.mid.2.lora_conv1.lora_A.weight', 'encoder.mid.2.lora_conv1.lora_B.weight', 'encoder.mid.2.lora_conv2.lora_A.weight', 'encoder.mid.2.lora_conv2.lora_B.weight']\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.0.res.0.lora_conv1.lora_A.weight - shape: torch.Size([4, 128, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.0.res.0.lora_conv1.lora_B.weight - shape: torch.Size([128, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.0.res.0.lora_conv2.lora_A.weight - shape: torch.Size([4, 128, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.0.res.0.lora_conv2.lora_B.weight - shape: torch.Size([128, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.0.res.1.lora_conv1.lora_A.weight - shape: torch.Size([4, 128, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.0.res.1.lora_conv1.lora_B.weight - shape: torch.Size([128, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.0.res.1.lora_conv2.lora_A.weight - shape: torch.Size([4, 128, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.0.res.1.lora_conv2.lora_B.weight - shape: torch.Size([128, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.1.res.0.lora_conv1.lora_A.weight - shape: torch.Size([4, 128, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.1.res.0.lora_conv1.lora_B.weight - shape: torch.Size([256, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.1.res.0.lora_conv2.lora_A.weight - shape: torch.Size([4, 256, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.1.res.0.lora_conv2.lora_B.weight - shape: torch.Size([256, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.1.res.0.lora_nin_shortcut.lora_A.weight - shape: torch.Size([4, 128, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.1.res.0.lora_nin_shortcut.lora_B.weight - shape: torch.Size([256, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.1.res.1.lora_conv1.lora_A.weight - shape: torch.Size([4, 256, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.1.res.1.lora_conv1.lora_B.weight - shape: torch.Size([256, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.1.res.1.lora_conv2.lora_A.weight - shape: torch.Size([4, 256, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.1.res.1.lora_conv2.lora_B.weight - shape: torch.Size([256, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.2.res.0.lora_conv1.lora_A.weight - shape: torch.Size([4, 256, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.2.res.0.lora_conv1.lora_B.weight - shape: torch.Size([256, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.2.res.0.lora_conv2.lora_A.weight - shape: torch.Size([4, 256, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.2.res.0.lora_conv2.lora_B.weight - shape: torch.Size([256, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.2.res.1.lora_conv1.lora_A.weight - shape: torch.Size([4, 256, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.2.res.1.lora_conv1.lora_B.weight - shape: torch.Size([256, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.2.res.1.lora_conv2.lora_A.weight - shape: torch.Size([4, 256, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.2.res.1.lora_conv2.lora_B.weight - shape: torch.Size([256, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.res.0.lora_conv1.lora_A.weight - shape: torch.Size([4, 256, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.res.0.lora_conv1.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.res.0.lora_conv2.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.res.0.lora_conv2.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.res.0.lora_nin_shortcut.lora_A.weight - shape: torch.Size([4, 256, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.res.0.lora_nin_shortcut.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.res.1.lora_conv1.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.res.1.lora_conv1.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.res.1.lora_conv2.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.res.1.lora_conv2.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.attn.0.lora_q.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.attn.0.lora_q.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.attn.0.lora_k.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.attn.0.lora_k.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.attn.0.lora_v.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.attn.0.lora_v.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.attn.1.lora_q.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.attn.1.lora_q.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.attn.1.lora_k.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.attn.1.lora_k.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.attn.1.lora_v.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.conv_blocks.3.attn.1.lora_v.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.0.lora_conv1.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.0.lora_conv1.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.0.lora_conv2.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.0.lora_conv2.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.1.lora_q.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.1.lora_q.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.1.lora_k.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.1.lora_k.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.1.lora_v.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.1.lora_v.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.2.lora_conv1.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.2.lora_conv1.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.2.lora_conv2.lora_A.weight - shape: torch.Size([4, 512, 1, 1])\n",
      "Uninitialized weight (requires_grad=True): encoder.mid.2.lora_conv2.lora_B.weight - shape: torch.Size([512, 4, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "model = VQ_models['VQ-8'](\n",
    "    codebook_size=16384,\n",
    "    codebook_embed_dim=8,\n",
    "    commit_loss_beta=0.25,\n",
    "    entropy_loss_ratio=0.0,\n",
    "    dropout_p=0.0,\n",
    ")\n",
    "\n",
    "# Load the weights\n",
    "weights = torch.load('vq_ds8_c2i.pt')\n",
    "missing_keys, unexpected_keys = model.load_state_dict(weights['model'], strict=False)\n",
    "\n",
    "# Print missing keys\n",
    "if missing_keys:\n",
    "    print(f\"Missing keys (weights not loaded): {missing_keys}\")\n",
    "\n",
    "# Print unexpected keys\n",
    "if unexpected_keys:\n",
    "    print(f\"Unexpected keys (weights in checkpoint but not in model): {unexpected_keys}\")\n",
    "\n",
    "# Identify and print the uninitialized weights\n",
    "for name, param in model.named_parameters():\n",
    "    if name in missing_keys:\n",
    "        param.requires_grad = True  # Enable gradient computation for uninitialized weights\n",
    "        print(f\"Uninitialized weight (requires_grad=True): {name} - shape: {param.shape}\")\n",
    "    else:\n",
    "        param.requires_grad = False \n",
    "        \n",
    "del weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighLowResDataset(Dataset):\n",
    "    def __init__(self, high_res_dir, low_res_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            high_res_dir (str): Directory with high-resolution images.\n",
    "            low_res_dir (str): Directory with low-resolution images.\n",
    "            transform (callable, optional): A function/transform to apply to both high and low-resolution images.\n",
    "        \"\"\"\n",
    "        self.high_res_dir = high_res_dir\n",
    "        self.low_res_dir = low_res_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # List of all image files in the high resolution directory\n",
    "        self.high_res_files = sorted([f for f in os.listdir(high_res_dir) if os.path.isfile(os.path.join(high_res_dir, f))])\n",
    "        self.low_res_files = sorted([f for f in os.listdir(low_res_dir) if os.path.isfile(os.path.join(low_res_dir, f))])\n",
    "        \n",
    "        if len(self.high_res_files) != len(self.low_res_files):\n",
    "            raise ValueError(\"The number of high-resolution and low-resolution images must be the same.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.high_res_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        high_res_file = self.high_res_files[idx]\n",
    "        low_res_file = self.low_res_files[idx]\n",
    "        \n",
    "        high_res_path = os.path.join(self.high_res_dir, high_res_file)\n",
    "        low_res_path = os.path.join(self.low_res_dir, low_res_file)\n",
    "        \n",
    "        high_res_image = Image.open(high_res_path).convert('RGB')\n",
    "        low_res_image = Image.open(low_res_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            high_res_image = self.transform(high_res_image)\n",
    "            low_res_image = self.transform(low_res_image)\n",
    "        \n",
    "        return high_res_image, low_res_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda pil_image: pil_image.resize((1024, 1024), Image.BICUBIC)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HighLowResDataset(high_res_dir='archive/lol_dataset/train/high', low_res_dir='archive/lol_dataset/train/low', transform=transform)\n",
    "loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 93184 || all params: 70203019 || trainable%: 0.13\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "    \n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-11-06 20:08:21\u001b[0m] Experiment directory created at experiments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from /media/mlr_lab/325C37DE7879ABF2/LowLIGHTQuantGAN/cache/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/243 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 23.64 GiB total capacity; 13.10 GiB already allocated; 175.81 MiB free; 13.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m optimizer_disc\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m recons_imgs, codebook_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhigh_res_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m loss_gen \u001b[38;5;241m=\u001b[39m vq_loss(\n\u001b[1;32m     55\u001b[0m     codebook_loss, low_res_image, recons_imgs, \n\u001b[1;32m     56\u001b[0m     optimizer_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, global_step\u001b[38;5;241m=\u001b[39mtrain_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     57\u001b[0m     last_layer\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mlast_layer, logger\u001b[38;5;241m=\u001b[39mlogger, log_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Backward pass for generator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlnhamtv0/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/mlr_lab/325C37DE7879ABF2/LowLIGHTQuantGAN/vq_model.py:59\u001b[0m, in \u001b[0;36mVQModel.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     quant, diff, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(quant)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec, diff\n",
      "File \u001b[0;32m/media/mlr_lab/325C37DE7879ABF2/LowLIGHTQuantGAN/vq_model.py:43\u001b[0m, in \u001b[0;36mVQModel.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 43\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_conv(h)\n\u001b[1;32m     45\u001b[0m     quant, emb_loss, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantize(h)\n",
      "File \u001b[0;32m~/anaconda3/envs/vlnhamtv0/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/mlr_lab/325C37DE7879ABF2/LowLIGHTQuantGAN/vq_model.py:109\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 109\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# downsampling\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i_level, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_blocks):\n",
      "File \u001b[0;32m~/anaconda3/envs/vlnhamtv0/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/vlnhamtv0/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlnhamtv0/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 23.64 GiB total capacity; 13.10 GiB already allocated; 175.81 MiB free; 13.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "from logger import create_logger\n",
    "\n",
    "# Ensure the scaler is set properly for mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "scaler_disc = torch.cuda.amp.GradScaler(enabled=True)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "params_to_update = [param for param in model.parameters() if param.requires_grad]\n",
    "vq_loss = VQLoss(\n",
    "    disc_start=20000,\n",
    "    disc_weight=0.5,\n",
    "    disc_type='patchgan',\n",
    "    disc_loss='hinge',\n",
    "    gen_adv_loss='hinge',\n",
    "    image_size=256,\n",
    "    perceptual_weight=1.0,\n",
    "    reconstruction_weight=1.0,\n",
    "    reconstruction_loss='l2',\n",
    "    codebook_weight=1.0,\n",
    ").to(device)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=1e-4, betas=(0.9, 0.95))\n",
    "optimizer_disc = torch.optim.Adam(vq_loss.discriminator.parameters(), lr=1e-4, betas=(0.9, 0.95))\n",
    "checkpoint_dir = 'checkpoints'\n",
    "experiment_dir = 'experiments'\n",
    "running_loss = 0\n",
    "log_steps = 0\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "vq_loss.train()\n",
    "train_steps = 0\n",
    "logger = create_logger(experiment_dir)\n",
    "logger.info(f\"Experiment directory created at {experiment_dir}\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(100):\n",
    "    with tqdm(total=len(loader), desc=f\"Epoch {epoch+1}/{100}\", unit=\"batch\") as pbar:\n",
    "        for i, (high_res_image, low_res_image) in enumerate(loader):\n",
    "            high_res_image = high_res_image.to(device)\n",
    "            low_res_image = low_res_image.to(device)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_disc.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            recons_imgs, codebook_loss = model(high_res_image)\n",
    "            loss_gen = vq_loss(\n",
    "                codebook_loss, low_res_image, recons_imgs, \n",
    "                optimizer_idx=0, global_step=train_steps + 1, \n",
    "                last_layer=model.decoder.last_layer, logger=logger, log_every=100\n",
    "            )\n",
    "            \n",
    "            # Backward pass for generator\n",
    "            scaler.scale(loss_gen).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(params_to_update, 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Backward pass for discriminator\n",
    "            loss_disc = vq_loss(\n",
    "                codebook_loss, low_res_image, recons_imgs, \n",
    "                optimizer_idx=1, global_step=train_steps + 1, \n",
    "                logger=logger, log_every=100\n",
    "            )\n",
    "            scaler_disc.scale(loss_disc).backward()\n",
    "            scaler_disc.unscale_(optimizer_disc)\n",
    "            torch.nn.utils.clip_grad_norm_(vq_loss.discriminator.parameters(), 1.0)\n",
    "            scaler_disc.step(optimizer_disc)\n",
    "            scaler_disc.update()\n",
    "\n",
    "            # Log loss values:\n",
    "            running_loss += loss_gen.item() + loss_disc.item()\n",
    "            log_steps += 1\n",
    "            train_steps += 1\n",
    "\n",
    "            # Update tqdm bar with the current batch information\n",
    "            pbar.set_postfix(\n",
    "                {\"Loss Gen\": f\"{loss_gen.item():.4f}\", \"Loss Disc\": f\"{loss_disc.item():.4f}\"}\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "            if train_steps % 100 == 0:\n",
    "                # Measure training speed:\n",
    "                end_time = time.time()\n",
    "                steps_per_sec = log_steps / (end_time - start_time)\n",
    "                avg_loss = running_loss / log_steps\n",
    "                logger.info(f\"(step={train_steps:07d}) Train Loss: {avg_loss:.4f}, Train Steps/Sec: {steps_per_sec:.2f}\")\n",
    "                running_loss = 0\n",
    "                log_steps = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "            # Save checkpoint:\n",
    "            if train_steps % 5000 == 0 and train_steps > 0:\n",
    "                model_weight = model.state_dict()\n",
    "                checkpoint = {\n",
    "                    \"model\": model_weight,\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"discriminator\": vq_loss.discriminator.state_dict(),\n",
    "                    \"optimizer_disc\": optimizer_disc.state_dict(),\n",
    "                    \"steps\": train_steps,\n",
    "                }\n",
    "                checkpoint_path = f\"{checkpoint_dir}/{train_steps:07d}.pt\"\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "                logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlnhamtv0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
